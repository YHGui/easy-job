## 概念

- 海量数据处理：基于海量数据的存储，处理和操作，海量数据一般难以一次性装载入内存，或者短时间难以迅速解决。

## 解决方法

### 哈希分治

- 方法介绍
  - 无法一次性将海量数据装载入内存，因此考虑将数据通过hash映射，分割成相应的小数据块，然后再针对各个数据块通过hashmap进行统计或者其他操作。
- 实例
  - 寻找访问Top IP：
    1. 分而治之：提取日志IP后，根据Hash映射将IP分到1000个小文件中，
    2. Hashmap统计：使用Hashmap统计频次
    3. 堆排序／快排：构建小顶堆进行排序求处最高频次IP，或者依据快排思想，每次排除一半数，最后找出最大数。
  - 寻找热门查询关键字：
    1. 查询记录过千万，首先去重，然后将其放入HashMap中，统计频次。
  - 寻找频数最高的100个词
    1. 将1G大小的文件，每行一个词，大小不超过16字节，内存限制为1M，返回频数最高的100个词。
    2. 分而治之或者hash映射进行分块，然后对每个小文件统计词频。
  - 寻找100台电脑上的Top10
    1. 如果同一数据元素只在某一台电脑，那么直接使用对排序分别求出100台电脑每台电脑的Top10
    2. 将100台电脑的Top10归并，最后求出整体Top10
    3. 如果数据分布在不同的机器上，那么可以先重新hash取模，使得每个元素都出现在同一台电脑，然后再按照上述方法进行求Top10
  - 寻找共同的url
    1. a和b两个文件夹，各存放50亿url，每个url占64字节，内存限制4G，找出共同url
    2. 首先对ab文件分别进行hash映射，然后存放到1000个文件中，那么可能相同的url都放到同一文件夹中了。
    3. 然后使用hashset完成这项工作
  - 10亿个数中找出最大的1000个数:分治+堆排序
    - 分治:10亿数太多加载不进内存,分配到多台机器中
    - 堆排序:将数放进小顶堆,如果比堆顶数小,丢弃;如果比堆顶数大,取而代之,然后重新调整
    - 最后将每组1000个数合并
  - 一个网站有100亿个url,存在一个黑名单,每个url平均64字节,这个黑名单要怎么存?若此时随便输入一个url,如何判断该url是否存在这个黑名单中?
    - bloomfilter(bit 数组+ k个hash函数)
    - 当输入一个url时,经过k个hash函数处理,得到多个hash值,hash值分别对数组长度m取模得到在数组的下标位置,将这些位置置为1
    - 如何判断一个url在黑名单?输入一条url,经过上述处理后,得到多个数组的下标位置,如果这些下标位置的元素值都已经为1了,说明该在黑名单中,否则不在.
    - 缺点:存在误判.但是如果某个元素不在,那么一定不在.